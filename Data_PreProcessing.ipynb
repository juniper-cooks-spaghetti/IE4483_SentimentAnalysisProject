{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\isaac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  sentiments\n",
      "0  I bought this belt for my daughter in-law for ...           1\n",
      "1  The size was perfect and so was the color.  It...           1\n",
      "2  Fits and feels good, esp. for doing a swim rac...           1\n",
      "3  These socks are absolutely the best. I take pi...           1\n",
      "4  Thank you so much for the speedy delivery they...           1\n",
      "                                             reviews\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...\n",
      "1  I dare say these are just about the sexiest th...\n",
      "2  everything about the transaction (price, deliv...\n",
      "3  Not bad for just a shirt.  Very durable, and m...\n",
      "4  These are truly wrinkle free and longer than t...\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_json('train.json')\n",
    "test_data = pd.read_json('test.json')\n",
    "\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stop words\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    # Remove extra whitespaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cleaned_reviews'] = train_data['reviews'].apply(clean_text)\n",
    "test_data['cleaned_reviews'] = test_data['reviews'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  \\\n",
      "0  I bought this belt for my daughter in-law for ...   \n",
      "1  The size was perfect and so was the color.  It...   \n",
      "2  Fits and feels good, esp. for doing a swim rac...   \n",
      "3  These socks are absolutely the best. I take pi...   \n",
      "4  Thank you so much for the speedy delivery they...   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0         bought belt daughter inlaw christmas loved  \n",
      "1            size perfect color looked like web page  \n",
      "2  fits feels good esp swim race highly recommend...  \n",
      "3  socks absolutely best take pilates classes hot...  \n",
      "4  thank much speedy delivery came time rehearsal...  \n"
     ]
    }
   ],
   "source": [
    "print(train_data[['reviews', 'cleaned_reviews']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (7401, 518)\n",
      "Test padded shape: (1851, 518)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['cleaned_reviews'])\n",
    "\n",
    "# Convert text to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_data['cleaned_reviews'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_data['cleaned_reviews'])\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in test_sequences))\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Display the shape of padded sequences\n",
    "print(f'Train padded shape: {train_padded.shape}')\n",
    "print(f'Test padded shape: {test_padded.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors from GloVe.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe word embeddings (download and extract glove.6B.100d.txt)\n",
    "embedding_dim = 100\n",
    "glove_file = 'glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_file, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Loaded {len(embeddings_index)} word vectors from GloVe.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (16366, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map the words in the tokenizer's vocabulary to GloVe vectors\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector  # Words found in GloVe will use pretrained vectors\n",
    "\n",
    "# Check the embedding matrix shape\n",
    "print(f'Embedding matrix shape: {embedding_matrix.shape}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
