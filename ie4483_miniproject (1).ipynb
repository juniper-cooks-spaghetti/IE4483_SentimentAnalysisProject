{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation_map = {\n",
    "    \"u\": \"you\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"idk\": \"I don't know\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"pls\": \"please\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"im\": \"I am\",\n",
    "    \"dont\": \"do not\",\n",
    "    \"cant\": \"cannot\",\n",
    "    \"wont\": \"will not\",\n",
    "    # Add more as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  sentiments\n",
      "0  I bought this belt for my daughter in-law for ...           1\n",
      "1  The size was perfect and so was the color.  It...           1\n",
      "2  Fits and feels good, esp. for doing a swim rac...           1\n",
      "3  These socks are absolutely the best. I take pi...           1\n",
      "4  Thank you so much for the speedy delivery they...           1\n",
      "                                             reviews\n",
      "0  I bought 2 sleepers.  sleeper had holes in the...\n",
      "1  I dare say these are just about the sexiest th...\n",
      "2  everything about the transaction (price, deliv...\n",
      "3  Not bad for just a shirt.  Very durable, and m...\n",
      "4  These are truly wrinkle free and longer than t...\n"
     ]
    }
   ],
   "source": [
    "#Changed the 'train_data' into 'raw_data' and changed 'test_data' into 'answer_data'\n",
    "\n",
    "labelled_data = pd.read_json('train.json')\n",
    "answer_data = pd.read_json('test.json')\n",
    "\n",
    "print(labelled_data.head())\n",
    "print(answer_data.head())\n",
    "\n",
    "raw_labels = labelled_data['sentiments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Expand abbreviations\n",
    "    text = ' '.join([abbreviation_map.get(word, word) for word in text.split()])\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords and lemmatize\n",
    "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data['cleaned_reviews'] = labelled_data['reviews'].apply(clean_text)\n",
    "answer_data['cleaned_reviews'] = answer_data['reviews'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             reviews  sentiments  \\\n",
      "0  I bought this belt for my daughter in-law for ...           1   \n",
      "1  The size was perfect and so was the color.  It...           1   \n",
      "2  Fits and feels good, esp. for doing a swim rac...           1   \n",
      "3  These socks are absolutely the best. I take pi...           1   \n",
      "4  Thank you so much for the speedy delivery they...           1   \n",
      "\n",
      "                                     cleaned_reviews  \n",
      "0         bought belt daughter inlaw christmas loved  \n",
      "1            size perfect color looked like web page  \n",
      "2  fits feels good esp swim race highly recommend...  \n",
      "3  socks absolutely best take pilates classes hot...  \n",
      "4  thank much speedy delivery came time rehearsal...  \n"
     ]
    }
   ],
   "source": [
    "print(labelled_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train padded shape: (7401, 518)\n",
      "Test padded shape: (1851, 518)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization: Basically divides the sentences into segments\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(labelled_data['cleaned_reviews']) #Indexes each token (each word is given a number). Index = 0 is for padding\n",
    "\n",
    "# Convert text to sequences\n",
    "raw_sequences = tokenizer.texts_to_sequences(labelled_data['cleaned_reviews'])\n",
    "answer_sequences = tokenizer.texts_to_sequences(answer_data['cleaned_reviews'])\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(max(len(seq) for seq in raw_sequences), max(len(seq) for seq in answer_sequences)) #Finding the maximum sequence length from both the raw_data and answer_data\n",
    "raw_final = pad_sequences(raw_sequences, maxlen=max_length, padding='post') #standardize the array\n",
    "answer_final = pad_sequences(answer_sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Display the shape of padded sequences\n",
    "print(f'Train padded shape: {raw_final.shape}')\n",
    "print(f'Test padded shape: {answer_final.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,  288,  214, ...,    0,    0,    0],\n",
       "       [   5,   39,   41, ...,    0,    0,    0],\n",
       "       [  86,  323,    7, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [1309,  401,  217, ...,    0,    0,    0],\n",
       "       [ 143,  957,  380, ...,    0,    0,    0],\n",
       "       [ 230,  906,   97, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  12,  116, 7406, ...,    0,    0,    0],\n",
       "       [3212,  123, 5234, ...,    0,    0,    0],\n",
       "       [ 172, 1543,   18, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  15,  108,  211, ...,    0,    0,    0],\n",
       "       [ 214,  132,   53, ...,    0,    0,    0],\n",
       "       [3179,  917,  153, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors from GloVe.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Load GloVe word embeddings (download and extract glove.6B.100d.txt)\n",
    "embedding_dim = 100 #as vectors of n real numbers\n",
    "glove_file = 'C:/Users/User/Downloads/Telegram Desktop/glove.6B/glove.6B.100d.txt' #change this to your glove path\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_file, encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f'Loaded {len(embeddings_index)} word vectors from GloVe.')\n",
    "\n",
    "# Visualize the GloVe embeddings with t-SNE\n",
    "\n",
    "# Step 1: Select a subset of words from the vocabulary (e.g., the first 100 words)\n",
    "word_subset = list(tokenizer.word_index.keys())[:100]\n",
    "word_vectors = np.array([embedding_matrix[tokenizer.word_index[word]] for word in word_subset])\n",
    "\n",
    "# Step 2: Apply t-SNE to reduce dimensionality to 2D\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "word_vecs_2d = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# Step 3: Plot the words in 2D space\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, word in enumerate(word_subset):\n",
    "    plt.scatter(word_vecs_2d[i, 0], word_vecs_2d[i, 1])\n",
    "    plt.annotate(word, (word_vecs_2d[i, 0], word_vecs_2d[i, 1]), fontsize=9)\n",
    "plt.title(\"2D Visualization of GloVe Embeddings (subset of vocabulary)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (16366, 100)\n"
     ]
    }
   ],
   "source": [
    "# Prepare the embedding matrix\n",
    "vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# Map the words in the tokenizer's vocabulary to GloVe vectors\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector  # Words found in GloVe will use pretrained vectors\n",
    "\n",
    "# Check the embedding matrix shape\n",
    "print(f'Embedding matrix shape: {embedding_matrix.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "518"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split the raw data\n",
    "from sklearn.model_selection import train_test_split\n",
    "labelled_train, labelled_test, label_train, label_test = train_test_split(\n",
    "    raw_final, raw_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 518, 100)          1636600   \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 518, 128)         84480     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 518, 128)          0         \n",
      "                                                                 \n",
      " bidirectional_9 (Bidirectio  (None, 8)                4256      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,725,345\n",
      "Trainable params: 1,725,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "19/19 [==============================] - 20s 541ms/step - loss: 0.4915 - acc: 0.8425 - val_loss: 0.3864 - val_acc: 0.8682\n",
      "Epoch 2/20\n",
      "19/19 [==============================] - 8s 409ms/step - loss: 0.4493 - acc: 0.8486 - val_loss: 0.3866 - val_acc: 0.8682\n",
      "Epoch 3/20\n",
      "19/19 [==============================] - 8s 410ms/step - loss: 0.4341 - acc: 0.8486 - val_loss: 0.3620 - val_acc: 0.8682\n",
      "Epoch 4/20\n",
      "19/19 [==============================] - 8s 410ms/step - loss: 0.3961 - acc: 0.8486 - val_loss: 0.3279 - val_acc: 0.8682\n",
      "Epoch 5/20\n",
      "19/19 [==============================] - 8s 413ms/step - loss: 0.3592 - acc: 0.8520 - val_loss: 0.2981 - val_acc: 0.8682\n",
      "Epoch 6/20\n",
      "19/19 [==============================] - 8s 406ms/step - loss: 0.3284 - acc: 0.8625 - val_loss: 0.2764 - val_acc: 0.8691\n",
      "Epoch 7/20\n",
      "19/19 [==============================] - 8s 407ms/step - loss: 0.2863 - acc: 0.8824 - val_loss: 0.2664 - val_acc: 0.8961\n",
      "Epoch 8/20\n",
      "19/19 [==============================] - 8s 412ms/step - loss: 0.2628 - acc: 0.9016 - val_loss: 0.2560 - val_acc: 0.8961\n",
      "Epoch 9/20\n",
      "19/19 [==============================] - 8s 424ms/step - loss: 0.2485 - acc: 0.9050 - val_loss: 0.2574 - val_acc: 0.9037\n",
      "Epoch 10/20\n",
      "19/19 [==============================] - 8s 425ms/step - loss: 0.2311 - acc: 0.9174 - val_loss: 0.2882 - val_acc: 0.8649\n",
      "Epoch 11/20\n",
      "19/19 [==============================] - 8s 413ms/step - loss: 0.2289 - acc: 0.9174 - val_loss: 0.2477 - val_acc: 0.9003\n",
      "Epoch 12/20\n",
      "19/19 [==============================] - 8s 408ms/step - loss: 0.2032 - acc: 0.9267 - val_loss: 0.2534 - val_acc: 0.9020\n",
      "Epoch 13/20\n",
      "19/19 [==============================] - 8s 404ms/step - loss: 0.1889 - acc: 0.9383 - val_loss: 0.2606 - val_acc: 0.9071\n",
      "Epoch 14/20\n",
      "19/19 [==============================] - 8s 410ms/step - loss: 0.1833 - acc: 0.9409 - val_loss: 0.2580 - val_acc: 0.9088\n",
      "Epoch 15/20\n",
      "19/19 [==============================] - 8s 405ms/step - loss: 0.1771 - acc: 0.9485 - val_loss: 0.2634 - val_acc: 0.9113\n",
      "Epoch 16/20\n",
      "19/19 [==============================] - 8s 405ms/step - loss: 0.1745 - acc: 0.9468 - val_loss: 0.2778 - val_acc: 0.9096\n",
      "Epoch 17/20\n",
      "19/19 [==============================] - 8s 408ms/step - loss: 0.1638 - acc: 0.9523 - val_loss: 0.2693 - val_acc: 0.9130\n",
      "Epoch 18/20\n",
      "19/19 [==============================] - 8s 410ms/step - loss: 0.1537 - acc: 0.9611 - val_loss: 0.2907 - val_acc: 0.9122\n",
      "Epoch 19/20\n",
      "19/19 [==============================] - 8s 411ms/step - loss: 0.1513 - acc: 0.9592 - val_loss: 0.2695 - val_acc: 0.9071\n",
      "Epoch 20/20\n",
      "19/19 [==============================] - 8s 405ms/step - loss: 0.1479 - acc: 0.9578 - val_loss: 0.2623 - val_acc: 0.9062\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout\n",
    "\n",
    "\n",
    "# Define your model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=True))\n",
    "#CuDNN has very strict requirements to be able to use GPU, im putting it here to show the requirements, some of them are already default settings though\n",
    "model.add(Bidirectional(LSTM(64,\n",
    "                             activation='tanh',  # Default settings, to show\n",
    "                             recurrent_activation='sigmoid',  # Default\n",
    "                             return_sequences=True,  # Must be set to true for the first layer\n",
    "                             recurrent_dropout=0,  # Must be 0 for cuDNN\n",
    "                             unroll=False,  # Must be False for cuDNN\n",
    "                             use_bias=True)))  # Default is True\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Bidirectional(LSTM(4,\n",
    "                             activation='tanh',  # Default settings, to show\n",
    "                             recurrent_activation='sigmoid',  # Default\n",
    "                             return_sequences=False,  # Set appropriately, not Default\n",
    "                             recurrent_dropout=0,  # Must be 0 for cuDNN\n",
    "                             unroll=False,  # Must be False for cuDNN\n",
    "                             use_bias=True)))  # Default is True\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "snn_model_history = model.fit(labelled_train, label_train, batch_size=256, epochs=20, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 5s 98ms/step - loss: 0.2617 - acc: 0.9041\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(labelled_test, label_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
